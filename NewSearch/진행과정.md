### django searchweb ver1 

#### 1. 구현
- [x] django restframework 적용
- [x] product,index model 생성

#### 2. 테스트 
- [x] test data 가지고 검색웹 확인

#### 문제점 
- token에 없는 키워드로 검색했을때 예외발생

--- 

### django searchweb ver2 

#### 1. 구현
- [x]  절대경로 처리 : 상대경로로 수정
- [x]  토크나이저 로직수정 : -들어있는거 하나 토큰으로 처리하기
- [x]  결측치 확인 : ifnull 확인
- [x]  tf-idf의 feature와 직접 token화 단어가 다름 (-가 사라짐) 
- 원인 : 사이킷런 내부에서 지정한 단어사전으로 만들어서 디폴트 토큰화
- 해결 : TfidfVectorizer의 tokenizer옵션 적용으로 해결

#### 2. 예외처리 
- [x]  검색한 키워드가 토큰에 없는 값이면? (아이폰) , (아이폰 맥북)
- [x]  검색한 키워드의 토큰리스트 값 중에 하나만 없는 토큰에 없는값이면?(아이폰 중고폰)
- [x]  교집합이 없으면 어떻게? (갤럭시 지갑)

#### 3. 성능효율
- [x]  raw data를 pd.read_csv로 읽으면 느림  → pyarrow로 더 빠르게 읽기
- [x]  계속 invert_index를 단순 dict로 가지고 있기엔 너무 큼  → df로 바꿔서 parquet파일로 저장
- [x]  계속 tf-idf matrix를 단순 array로 가지고 있기엔 너무 큼 → df로 바꿔서 parquet파일로 저장
- [ ]  초기세팅(data load)이 너무 느림 
- 해결1 : 처음에 pd.read_csv 말고 pyarrow로 rawdata 파일읽기
- 해결2 : 읽어온 데이터를 bulk create로 모델에 저장 -> 이게 너무 오래걸림
- 해결3 : 이후 모델의 값을 읽어서 (object.values.all) df로 사용

#### 4. 테스트
- [x] test data 가지고 검색웹 확인
- [ ] real data 가지고 검색웹 확인 -> 데이터파일을 bulk create로 '모델'에 저장하는 방식

#### 문제점 
- 10만건의 데이터를 pyarrow로 읽고, bulk create해서 모델(sqlite)에 넣는게 엄청엄청 오래걸림

--- 
### django searchweb ver3
#### 1. 구현

#### 2. 테스트
- [x] test data 가지고 검색웹 확인
- [x] real data 가지고 검색웹 확인 -> 데이터파일을 rawdata.parquet '파일'에 저장하는 방식
- [ ] tf-idf로 계산한 score값이 확실한가??

#### 3. 성능효율
- [ ]  초기세팅(data load)이 조금 느림 
- 해결1 : 처음에 pyarrow로 rawdata 파일읽기
- 해결2 : 모델없이 rawdata.parquet 파일로 디렉토리에 저장 -> 최대1분 소요 
- 해결3 : 이후 parquet파일을 read해서 df로 사용 -> 최대10초 소요
- [ ] tf-idf계산 메모리이슈 : tf-idf matrix생성시 로컬python에서 메모리부족 문제 발생 
- 임시해결 : 가장 많이나온단어(max_features) 기준으로 100개 features 사용 
- 임시해결 : 없는 features는 tf-idf score 합계에 포함되지 않음

#### 4. 예외처리 
- [x] max_features 옵션을 적용하다보니 검색한 키워드의 토큰이 tf-idf feature에 없는 상황 발생
- 임시해결 : 검색한 키워드의 토큰이 tfidv_df의 column에 포함되는지 확인 

#### 문제점
- parquet파일 read/write방식 : 초기세팅 최대1분, 이후검색 10초소요
- 로컬파일 : parquet파일들을 내부 디렉토리에 저장해서 파일명 및 경로를 지정해줘야함
- 메모리부족 : 전체 token을 feature로 갖는 tf-idf.parquet 파일생성 불가 -> s3에 저장하기?
- 모든 feature 사용하지 않음 :  없는 features는 tf-idf score 합계에 포함되지 않아서 score 부정확

---
### django searchweb ver4
#### 1. 구현

#### 2. 테스트
- [x] real data 가지고 검색웹 확인 : 톰브라운 정품 맨투맨
- [ ] real data 가지고 검색웹 확인 : 방탄 정국포카 -> "id": 153823277의 score가 null

#### 3. 성능효율
- [x] tf-idf계산 메모리이슈 해결 
- 원인 : 검색하기전에 미리 모든문서에 대한 tf-idf.parquet 파일을 생성해서 메모리이슈 발생
- 해결 : 검색된 문서id에 대해서만 tf-idf를 계산하고, 검색하도록 수정   
- [ ] 초기세팅(data load)이 조금 느림 -> 최대 1분 
- [ ] 더 효율적인 방법 고민해보기 
    - 로컬파일이 아니라 s3에 저장해서 read/write 한다면?
    - spark로 처리해야하나? spark를 쓴다면 SparkContext를 언제 시작? SparkContext를 인스턴스화 싱글톤?

#### 4. 예외처리
- [ ] 교집합 문서ID가 없거나 올바르지 않은 검색어일 경우 

#### 문제점
- parquet파일 read/write방식 : 초기세팅 최대1분, 이후검색 10초소요
- score에 null값이 생기는 경우가 있음 

--- 
### django searchweb ver5
#### 1. 구현
- [x] 상위20개 결과 출력하기
- [x] score null 문제 -> concat이 아닌 id기준 merge로 해결

#### 2. (예외처리) 테스트
- [x] real data 가지고 검색웹 확인 : 톰브라운 정품 맨투맨
- [x] real data 가지고 검색웹 확인 : 방탄 정국포카 -> null 없이 상위20개 확인 
- [x] 교집합 문서ID가 없는 경우 : 방탄 맥북m1 -> 예외처리
- [x] 전부 올바르지 않은 검색어일 경우 : 포키포키 -> 예외처리
- [x] 올바르지 않은 검색어가 있을 경우 : 포키포키 포카양도-> 있는 키워드(포카양도)에 대해서만 검색

#### 3. 성능효율
- [x] tf-idf계산 메모리이슈 해결 -> 초기세팅시 생성한 tfidf.parqet 파일삭제

#### 4. 결론
- 초기검색 
  - data load : /data/proct_name.tsv 파일을 pyarrow로 읽어서 data.parquet로 저장
  - invertIndex : 구현한 tokenizer로 분리된 token이 key, 문서id리스트가 value를 index.parquet로 저장
- 검색
  - index.parquet을 읽서 dataframe(index_df)로 사용 -> 매칭되는 문서ID리스트의 교집합ID검색
  - data.parquet을 읽어 dataframe(data_df)로 사용 -> 검색된 교집합ID의 id,name 저장
  - 사이킷런의 TfidfVectorizer 사용해서 dataframe(tfidf_df)로 사용 -> 검색된 교집합ID의 id,score 저장
  - Response : score기준으로 정렬된 상위20개 검색결과(id,name,score)
- 한계점 
  - 초기세팅(data load)이 조금 느림 -> 최대 1분, 하지만 parquet파일이 load 된 상태라면 검색빠름
  - 로컬파일 : parquet파일들은 내부 로컬리렉토리에서 read/write진행

---
#### 기타. 코드정리
- [x] 주석달기
- [x] 모듈화 : class 생성
- [x] postman으로 API 테스트진행
- [x] 사용 패키지 : django, djangorestframework, sklearn, padnas, pyarrow, boto3, django-storages

#### 기타. 어려웠던점이나 느낀점
- konly 자연어 패키지나 elasticSearch 오픈소스로 쉽게 가려고하다가 오히려 시간낭비 -> 순수하게 해결방법 찾자
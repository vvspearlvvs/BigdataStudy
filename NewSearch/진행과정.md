### django searchweb ver1 

#### 1. 구현
- [x] django restframework 적용
- [x] product,index model 생성

#### 2. 테스트 
- [x] test data 가지고 검색웹 확인 : 성공
- [ ] real data 가지고 검색웹 확인 : 실패

#### 문제점 
- data read/load 불가 : 대규모데이터를 읽고 저장하는데 오래걸린다 -> 어떻게 빠르게 읽고 저장하지? 

--- 

### django searchweb ver2 

#### 1. 구현
- [x]  절대경로 처리 : 상대경로로 수정
- [x]  토크나이저 로직수정 : -들어있는거 하나 토큰으로 처리하기
- [x]  결측치 확인 : ifnull 확인
- [x]  tf-idf의 feature와 직접 token화 단어가 다름 (-가 사라짐) 
- 원인 : 사이킷런 내부에서 지정한 단어사전으로 만들어서 디폴트 토큰화
- 해결 : TfidfVectorizer의 tokenizer옵션 적용으로 해결

#### 2. 예외처리 
- [x]  검색한 키워드가 토큰에 없는 값이면? (아이폰) , (아이폰 맥북)
- [x]  검색한 키워드의 토큰리스트 값 중에 하나만 없는 토큰에 없는값이면?(아이폰 중고폰)
- [x]  교집합이 없으면 어떻게? (갤럭시 지갑)

#### 3. 성능효율 
- [x] data read 속도향상 : with open -> pd.read_csv → pyarrow read 변경
- [x] data load 속도향상 : model(sqlite)에 bulk create
- [x] index.parquet 저장 : 단순 invertIndex (dict변수) -> dataframe -> parquet파일로 저장
- [x] tf-idf.parquet 저장 : 단순 matrix (array변수) -> dataframe -> parquet파일로 저장
- [ ] 초기세팅(data load)이 너무 느림 
- 해결1 : 처음에 pd.read_csv 말고 pyarrow로 rawdata 파일읽기
- 해결2 : 읽어온 데이터를 bulk create로 모델에 저장 -> 이게 너무 오래걸림
- 해결3 : 이후 모델의 값을 읽어서 (object.values.all) df로 사용

#### 4. 테스트
- [x] test data 가지고 검색웹 확인 : 성공
- [ ] real data 가지고 검색웹 확인 : 실패 

#### 문제점 
- data load 불가 : pyarrow로 빠르게 읽어올 수 있지만, bulk crate로 모델(sqlite)에 저장하는데 엄청엄청 오래걸린다 -> 어디에 저장? 

--- 
### django searchweb ver3
#### 1. 구현

#### 2. 테스트
- [x] test data 가지고 검색웹 확인 : 성공
- [x] real data 가지고 검색웹 확인 : 성공 
  - 초기에 파일을 생성하는데 최대 1분소요 
  - 이후 파일을 가져와서 사용하는데 10초 이내로 동작 
  
#### 3. 성능효율
- [x] data load 속도향상 : model없이 rawdarta.parquet 파일에 저장, 이후 dataframe으로 사용

#### 문제점 
- 초기세팅(data load) 최대1분 소요, 하지만 parquet파일이 load 된 상태라면 검색빠름
- 사이킷런 tf-idf 계산시 메모리부족 문제 발생
  - 임시해결 : 가장 많이나온단어(max_features) 기준으로 100개 features 사용
  - 예외처리 : 검색한 키워드의 토큰이 tfidv_df의 column에 포함되는지 확인
  - 한계점 : 없는 features는 tf-idf score 합계에 포함되지 않아서 score 부정확

---
### django searchweb ver4
#### 1. 구현

#### 2. 테스트
- [x] real data 가지고 검색웹 확인 : 성공 (톰브라운 정품 맨투맨)
- [ ] real data 가지고 검색웹 확인 : 성공 (방탄 정국포카) 
  - 문제점발견 : score가 null인 값이 존재

#### 3. 성능효율
- [x] 사이킷런 tf-idf 계산시 메모리부족 문제 해결 
  - 해결 : 모든문서에 대해 미리 tf-idf계산 -> 검색된 문서id에 대해서만 계산하도록 수정
- [ ] 더 효율적인 방법 고민해보기 
    - 로컬파일이 아니라 s3에 저장해서 read/write 한다면?
    - spark로 처리해야하나? spark를 쓴다면 SparkContext를 언제 시작? SparkContext를 인스턴스화 싱글톤?

#### 문제점
- 초기세팅(data load) 최대1분 소요, 하지만 parquet파일이 load 된 상태라면 검색빠름
- score에 null값이 생기는 경우가 있음 

--- 
### django searchweb ver5
#### 1. 구현
- [x] 상위20개 결과 출력하기
- [x] score에 null값이 생기는 경우 concat이 아닌 id기준 merge로 해결
- [x] 교집합 문서ID가 없거나 올바르지 않은 검색어일 경우 예외처리

#### 2. 테스트
- [x] real data 가지고 검색웹 확인 : 톰브라운 정품 맨투맨
- [x] real data 가지고 검색웹 확인 : 방탄 정국포카 -> null 없이 상위20개 확인 
- [x] 교집합 문서ID가 없는 경우 : 방탄 맥북m1 -> 예외처리
- [x] 전부 올바르지 않은 검색어일 경우 : 포키포키 -> 예외처리
- [x] 올바르지 않은 검색어가 있을 경우 : 포키포키 포카양도-> 있는 키워드(포카양도)에 대해서만 검색

#### 3. 성능효율
- [ ] 더 효율적인 방법 고민해보기
  - 로컬파일이 아니라 s3에 저장해서 read/write 한다면?
  - spark로 처리해야하나? spark를 쓴다면 SparkContext를 언제 시작? SparkContext를 인스턴스화 싱글톤?
  
#### 4. 결론
- 초기검색 
  - data load : /data/proct_name.tsv 파일을 pyarrow로 읽어서 data.parquet로 저장
  - invertIndex : 구현한 tokenizer로 분리된 token이 key, 문서id리스트가 value를 index.parquet로 저장
- 검색
  - index.parquet을 읽서 dataframe(index_df)로 사용 -> 매칭되는 문서ID리스트의 교집합ID검색
  - data.parquet을 읽어 dataframe(data_df)로 사용 -> 검색된 교집합ID의 id,name 저장
  - 사이킷런의 TfidfVectorizer 사용해서 dataframe(tfidf_df)로 사용 -> 검색된 교집합ID의 id,score 저장
  - Response : score기준으로 정렬된 상위20개 검색결과(id,name,score)
- 한계점 
  - 초기세팅(data load) 최대1분 소요, 하지만 parquet파일이 load 된 상태라면 검색빠름
  - 로컬파일 : parquet파일들은 내부 로컬리렉토리에서 read/write진행

---
#### 기타. 코드정리
- [x] 주석달기
- [x] 모듈화 : DataSetting class, Mathching class 생성
- [x] postman으로 API 테스트진행
- [x] 사용 패키지 : django, djangorestframework, sklearn, padnas, pyarrow, boto3, django-storages

#### 기타. 어려웠던점이나 느낀점
- konly 자연어 패키지나 elasticSearch 오픈소스로 쉽게 가려고하다가 오히려 시간낭비 -> 순수하게 해결방법 찾자